{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffa1d92",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) – An Intuitive Guide\n",
    "\n",
    "## What is PCA?\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a method used to reduce the number of variables in a dataset while preserving as much of the original information as possible. It identifies patterns in the data and transforms the data into a new coordinate system based on those patterns.\n",
    "\n",
    "The new variables, called *principal components*, are uncorrelated and ordered by how much variance they capture from the original dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Analogy: The Shadow on the Wall\n",
    "\n",
    "Think of a 3D object like a cube. When light shines on it, it casts a 2D shadow on a wall. Depending on the angle of the light, some shadows preserve more of the cube’s shape than others.\n",
    "\n",
    "PCA does something similar. It projects high-dimensional data into fewer dimensions while trying to preserve as much of the \"shape\" (or variance) of the data as possible.\n",
    "\n",
    "* Original data = the 3D object  \n",
    "* Lower-dimensional representation = the shadow  \n",
    "* PCA = the process of finding the best angle for projection\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use PCA?\n",
    "\n",
    "### * Dimensionality Reduction\n",
    "\n",
    "* High-dimensional data can be hard to work with and may cause overfitting.\n",
    "* PCA reduces dimensions by finding a smaller set of uncorrelated variables.\n",
    "\n",
    "### * Noise Reduction\n",
    "\n",
    "* PCA filters out noise by focusing on the directions of greatest variation.\n",
    "* This helps keep the most important patterns and discard irrelevant information.\n",
    "\n",
    "### * Visualization\n",
    "\n",
    "* PCA makes it possible to visualize complex, high-dimensional data by projecting it down to 2D or 3D.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition Behind How PCA Works\n",
    "\n",
    "1. PCA looks for directions (called *components*) where the data varies the most.\n",
    "2. It defines new axes along those directions.\n",
    "3. The first principal component captures the most variance.\n",
    "4. The second captures the next most, at a right angle to the first, and so on.\n",
    "5. You can then project your data onto these new axes to get a compressed version.\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Intuition: Rotating the Axes\n",
    "\n",
    "Imagine looking at a cloud of points. The X and Y axes might not line up with the way the points are spread out. PCA rotates the axes to match the natural orientation of the data, so the new axes (components) describe the data more efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "* **Image compression** – Reduce image dimensions while preserving detail\n",
    "* **Genomics** – Identify dominant patterns in gene data\n",
    "* **Finance** – Detect main movements in market data\n",
    "* **Marketing** – Understand customer segments from behavior data\n",
    "\n",
    "---\n",
    "\n",
    "## Important Considerations\n",
    "\n",
    "* PCA creates new features by combining existing ones.\n",
    "* These new features may be hard to interpret directly.\n",
    "* PCA is best for data with linear relationships.\n",
    "* Data should usually be standardized before applying PCA (mean = 0, variance = 1).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Concept        | Description                                                  |\n",
    "|----------------|--------------------------------------------------------------|\n",
    "| Goal           | Reduce dimensionality while preserving important structure   |\n",
    "| Technique      | Find new uncorrelated variables (principal components)       |\n",
    "| Result         | Smaller number of features with most of the original variance|\n",
    "| Applications   | Compression, visualization, denoising, pattern detection     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd4e6e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce3c47",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Breast cancer wisconsin (diagnostic) dataset\n",
    "--------------------------------------------\n",
    "\n",
    "**Data Set Characteristics:**\n",
    "\n",
    "    :Number of Instances: 569\n",
    "\n",
    "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
    "\n",
    "    :Attribute Information:\n",
    "        - radius (mean of distances from center to points on the perimeter)\n",
    "        - texture (standard deviation of gray-scale values)\n",
    "        - perimeter\n",
    "        - area\n",
    "        - smoothness (local variation in radius lengths)\n",
    "        - compactness (perimeter^2 / area - 1.0)\n",
    "        - concavity (severity of concave portions of the contour)\n",
    "        - concave points (number of concave portions of the contour)\n",
    "        - symmetry\n",
    "        - fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "        worst/largest values) of these features were computed for each image,\n",
    "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
    "        10 is Radius SE, field 20 is Worst Radius.\n",
    "\n",
    "        - class:\n",
    "                - WDBC-Malignant\n",
    "                - WDBC-Benign\n",
    "\n",
    "    :Summary Statistics:\n",
    "\n",
    "    ===================================== ====== ======\n",
    "                                           Min    Max\n",
    "    ===================================== ====== ======\n",
    "    radius (mean):                        6.981  28.11\n",
    "    texture (mean):                       9.71   39.28\n",
    "    perimeter (mean):                     43.79  188.5\n",
    "    area (mean):                          143.5  2501.0\n",
    "    smoothness (mean):                    0.053  0.163\n",
    "    compactness (mean):                   0.019  0.345\n",
    "    concavity (mean):                     0.0    0.427\n",
    "    concave points (mean):                0.0    0.201\n",
    "    symmetry (mean):                      0.106  0.304\n",
    "    fractal dimension (mean):             0.05   0.097\n",
    "    radius (standard error):              0.112  2.873\n",
    "    texture (standard error):             0.36   4.885\n",
    "    perimeter (standard error):           0.757  21.98\n",
    "    area (standard error):                6.802  542.2\n",
    "    smoothness (standard error):          0.002  0.031\n",
    "    compactness (standard error):         0.002  0.135\n",
    "    concavity (standard error):           0.0    0.396\n",
    "    concave points (standard error):      0.0    0.053\n",
    "    symmetry (standard error):            0.008  0.079\n",
    "    fractal dimension (standard error):   0.001  0.03\n",
    "    radius (worst):                       7.93   36.04\n",
    "    texture (worst):                      12.02  49.54\n",
    "    perimeter (worst):                    50.41  251.2\n",
    "    area (worst):                         185.2  4254.0\n",
    "    smoothness (worst):                   0.071  0.223\n",
    "    compactness (worst):                  0.027  1.058\n",
    "    concavity (worst):                    0.0    1.252\n",
    "    concave points (worst):               0.0    0.291\n",
    "    symmetry (worst):                     0.156  0.664\n",
    "    fractal dimension (worst):            0.055  0.208\n",
    "    ===================================== ====== ======\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
    "\n",
    "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
    "\n",
    "    :Donor: Nick Street\n",
    "\n",
    "    :Date: November, 1995\n",
    "\n",
    "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
    "https://goo.gl/U2Uwz2\n",
    "\n",
    "Features are computed from a digitized image of a fine needle\n",
    "aspirate (FNA) of a breast mass.  They describe\n",
    "characteristics of the cell nuclei present in the image.\n",
    "\n",
    "Separating plane described above was obtained using\n",
    "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
    "Construction Via Linear Programming.\" Proceedings of the 4th\n",
    "Midwest Artificial Intelligence and Cognitive Science Society,\n",
    "pp. 97-101, 1992], a classification method which uses linear\n",
    "programming to construct a decision tree.  Relevant features\n",
    "were selected using an exhaustive search in the space of 1-4\n",
    "features and 1-3 separating planes.\n",
    "\n",
    "The actual linear program used to obtain the separating plane\n",
    "in the 3-dimensional space is that described in:\n",
    "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
    "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
    "Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "This database is also available through the UW CS ftp server:\n",
    "\n",
    "ftp ftp.cs.wisc.edu\n",
    "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
    "\n",
    ".. topic:: References\n",
    "\n",
    "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
    "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
    "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
    "     San Jose, CA, 1993.\n",
    "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
    "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
    "     July-August 1995.\n",
    "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
    "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
    "     163-171."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cancer_tumor_data_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b721453",
   "metadata": {},
   "source": [
    "## PCA with Scikit-Learn\n",
    "\n",
    "\n",
    "### Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74307a",
   "metadata": {},
   "source": [
    "## Scikit-Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using n_components as 2 will return 2 features from our dataset\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components = pca.fit_transform(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the PCA features using a scatter plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(principal_components[:,0],principal_components[:,1])\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3cf285",
   "metadata": {},
   "source": [
    "## Load the breast cancer dataset from scikit-learn to extract our target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRES INTERNET CONNECTION AND FIREWALL ACCESS\n",
    "cancer_dictionary = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dictionary['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the PCA features using a scatter plot and seperating the clusters using the target variable\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(principal_components[:,0],principal_components[:,1],c=cancer_dictionary['target'])\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b69e7",
   "metadata": {},
   "source": [
    "## Fitted Model Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the \"directions\" of the new axes created by PCA.\n",
    "# Each row is a principal component, and each number in the row shows how much the original feature contributes to that component.\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092697f",
   "metadata": {},
   "source": [
    "In this numpy matrix array, each row represents a principal component, Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_.\n",
    "\n",
    "We can visualize this relationship with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = pd.DataFrame(pca.components_,index=['PC1','PC2'],columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,3),dpi=150)\n",
    "sns.heatmap(df_comp,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the proportion of the dataset's total variance explained by each principal component.\n",
    "# Useful for understanding how much information is retained when reducing dimensions.\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using a n_component of 30\n",
    "pca_30 = PCA(n_components=30)\n",
    "pca_30.fit(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_30.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca_30.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = []\n",
    "\n",
    "for n in range(1,30):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(scaled_X)\n",
    "    \n",
    "    explained_variance.append(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a line chart displaying elbow method to chose the n_component that will be better.\n",
    "plt.plot(range(1,30),explained_variance)\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Variance Explained\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45abab31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419a68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f93d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17931684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
